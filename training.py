# ultimate_fix.py
import torch
from transformers import (
    AutoTokenizer, 
    TextDataset,
    LineByLineTextDataset,
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset
from inspect_tokenized_dataset import TokenizedDatasetInspector

import os
import logging
import numpy as np
import pandas as pd


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
train_path = "./simple_train.txt"
model_name ="./models/Qwen3-1.7B"
# model_name ="./ultra_safe_model"


def debug_labels_structure(labels, name="labels"):
    """æ·±åº¦è°ƒè¯•labelsç»“æ„"""
    logger.info(f"ğŸ” è°ƒè¯•{name}ç»“æ„:")
    logger.info(f"  ç±»å‹: {type(labels)}")
    
    if isinstance(labels, list):
        logger.info(f"  é•¿åº¦: {len(labels)}")
        if labels:
            first_item = labels[0]
            logger.info(f"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(first_item)}")
            
            if isinstance(first_item, list):
                logger.error("âŒ æ£€æµ‹åˆ°åµŒå¥—åˆ—è¡¨!")
                if first_item and isinstance(first_item[0], list):
                    logger.error("âŒ æ£€æµ‹åˆ°åŒé‡åµŒå¥—åˆ—è¡¨!")
                logger.info(f"    åµŒå¥—ç¤ºä¾‹: {labels[:2]}")
            elif isinstance(first_item, (int, np.integer)):
                logger.info("âœ… æ˜¯æ•´æ•°åˆ—è¡¨ - æ­£ç¡®æ ¼å¼")
            else:
                logger.warning(f"âš ï¸ åŒ…å« {type(first_item)} ç±»å‹å…ƒç´ ")
    else:
        logger.warning(f"âš ï¸ ä¸æ˜¯åˆ—è¡¨: {type(labels)}")


def prepare_line_based_dataset(tokenizer, file_path: str, max_length: int = 128) -> Dataset:
    """å‡†å¤‡åŸºäºè¡Œçš„è®­ç»ƒæ•°æ®é›†"""
    logger.info(f"ğŸ“š å‡†å¤‡è¡Œçº§è®­ç»ƒæ•°æ®: {file_path}")
        
    # åŠ è½½æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼‰
    dataset = load_dataset('text', data_files={'train': train_path})['train']
        
    # Tokenizeå‡½æ•°
    def tokenize_function(examples):
        logger.info(f"ğŸ“š tokenize_function: {examples}")
        # å¯¹æ¯è¡Œç‹¬ç«‹tokenize
        tokenized = tokenizer(
            examples['text'],
            truncation=True,      # æˆªæ–­åˆ°max_length
            padding="max_length", # å¡«å……åˆ°max_lengthï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
            max_length=128,
            return_tensors="pt"   # è¿”å›PyTorchå¼ é‡
        )
        return tokenized
        
    # åº”ç”¨tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=10,
        remove_columns=dataset.column_names,
        desc="Tokenizing lines for LM"
    )
        
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(tokenized_dataset)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return tokenized_dataset


def get_data_from_cvs(tokenizer) -> Dataset:
    try:
        # 1. è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv("./tran_data.csv")
        print(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")
        print(f"æ•°æ®åˆ—: {df.columns.tolist()}")
        
        # 2. æ£€æŸ¥å¿…è¦åˆ—
        if 'prompt' not in df.columns or 'response' not in df.columns:
            raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å«'prompt'å’Œ'response'åˆ—")
        
        # 3. åˆ›å»ºè®­ç»ƒæ–‡æœ¬
        train_texts = []
        for _, row in df.iterrows():
            # åˆ›å»ºæ ¼å¼åŒ–çš„å¯¹è¯æ–‡æœ¬
            text = f"ç”¨æˆ·: {row['prompt']}\nåŠ©æ‰‹: {row['response']}"
            train_texts.append({"text": text})
        
        # 4. åˆ›å»ºDataset
        dataset = Dataset.from_list(train_texts)
        print(f"åˆ›å»ºDatasetï¼Œå…± {len(dataset)} æ¡æ ·æœ¬")
        
        # 5. Tokenizeå‡½æ•°
        def tokenize_function(examples):
            """
            åˆ†è¯å¤„ç†å‡½æ•°
            """
            # Tokenizeæ–‡æœ¬
            tokenized = tokenizer(
                examples["text"],
                truncation=True,      # æˆªæ–­åˆ°max_length
                padding="max_length", # å¡«å……åˆ°max_lengthï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
                max_length=128,
                return_tensors="pt"   # è¿”å›PyTorchå¼ é‡
            )
            
            # å¯¹äºè¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œlabelsé€šå¸¸æ˜¯input_idsçš„å‰¯æœ¬
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        
        # 6. åº”ç”¨åˆ†è¯å‡½æ•°
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=10,
            remove_columns=["text"]  # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—ï¼ŒèŠ‚çœå†…å­˜
        )
        
        print(f"åˆ†è¯å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")
        return tokenized_dataset
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")
        raise
    except Exception as e:
        print(f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise


def ultra_safe_tokenize_and_train():
    """è¶…å®‰å…¨çš„tokenizationå’Œè®­ç»ƒæµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹è¶…å®‰å…¨è®­ç»ƒæµç¨‹")
    print("=" * 60)
    
    output_dir = "./ultra_safe_model"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡å‹
        logger.info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        logger.info("ğŸ“Š åˆ›å»ºTextDataset...")
        # train_dataset = LineByLineTextDataset(
        #     tokenizer=tokenizer,
        #     file_path=train_path,
        #     block_size=128  # åºåˆ—é•¿åº¦
        # )
        # train_dataset = prepare_line_based_dataset(tokenizer,train_path,128)
        train_dataset = get_data_from_cvs(tokenizer)

          # 2. åŸºç¡€æŸ¥çœ‹å™¨
        inspector = TokenizedDatasetInspector(tokenizer)
        
        # 3. åŸºç¡€æŸ¥çœ‹
        inspector.basic_inspection(train_dataset, num_samples=3)
        
        # 4. ç»Ÿè®¡åˆ†æ
        stats = inspector.statistical_analysis(train_dataset)
        
        # 5. è§£ç æ˜¾ç¤º
        inspector.decode_and_display(train_dataset, num_samples=2)
        

        logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°: {len(train_dataset)}")
        
        # 4. éªŒè¯æ•°æ®é›†ç»“æ„
        if len(train_dataset) > 0:
            sample = train_dataset[0]
            logger.info("ğŸ” éªŒè¯æ•°æ®é›†æ ·æœ¬ç»“æ„:")
            
            # æ£€æŸ¥æ ·æœ¬ç±»å‹å’Œç»“æ„
            logger.info(f"  æ ·æœ¬ç±»å‹: {type(sample)}")
            if hasattr(sample, 'keys'):
                logger.info(f"  æ ·æœ¬é”®: {list(sample.keys())}")
            else:
                # TextDatasetè¿”å›çš„æ˜¯å­—å…¸
                if isinstance(sample, dict):
                    for key, value in sample.items():
                        logger.info(f"  {key}: ç±»å‹={type(value)}")
                        if hasattr(value, '__len__'):
                            logger.info(f"   é•¿åº¦: {len(value)}")
                            debug_labels_structure(value, key)
                else:
                    logger.info(f"  æ ·æœ¬å€¼ç±»å‹: {type(sample)}")
                    if hasattr(sample, '__len__'):
                        logger.info(f"  æ ·æœ¬é•¿åº¦: {len(sample)}")
                        debug_labels_structure(sample, "æ ·æœ¬")
        
        # 5. é…ç½®æ•°æ®æ•´ç†å™¨ï¼ˆå…³é”®ï¼šè®©DataCollatorå¤„ç†labelsï¼‰
        logger.info("ğŸ”§ é…ç½®DataCollator...")
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,  # å› æœè¯­è¨€å»ºæ¨¡
        )
        

        # 6. è®­ç»ƒå‚æ•°ï¼ˆæœ€ç®€é…ç½®ï¼‰
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=4, #è®­ç»ƒè½®æ¬¡
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=5e-5, #é€šç”¨å­¦ä¹ ç‡
            save_steps=10,
            save_total_limit=1,
            weight_decay=0.01, 
            logging_steps=10,
            remove_unused_columns=False, #åˆ é™¤æ— æ•ˆçš„ç»„
            dataloader_pin_memory=False,
            # ç¦ç”¨å¯èƒ½å¼•èµ·é—®é¢˜çš„åŠŸèƒ½
            prediction_loss_only=True,  # è®©DataCollatorå¤„ç†
        )
        
        # 7. åˆ›å»ºè®­ç»ƒå™¨
        logger.info("ğŸ¯ åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
        )
        
        # 8. å¼€å§‹è®­ç»ƒ
        logger.info("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
        print("=" * 50)
        
        train_result = trainer.train()
        
        # 9. ä¿å­˜æ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        metrics = train_result.metrics
        logger.info("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        logger.info(f"ğŸ“Š æœ€ç»ˆæŸå¤±: {metrics.get('train_loss', 'N/A')}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ ç»ˆæä¿®å¤æ–¹æ¡ˆå¯åŠ¨")
    print("=" * 60)

    # create_simple_text_file()
    
    # è¿è¡Œè¶…å®‰å…¨è®­ç»ƒ
    success = ultra_safe_tokenize_and_train()
    
    if success:
        print("\n è®­ç»ƒæˆåŠŸ")
    else:
        print("\n è®­ç»ƒå¤±è´¥")