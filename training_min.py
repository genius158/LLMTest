# minimal_working.py
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments
import os

def create_text_file():
    """åˆ›å»ºæ–‡æœ¬æ–‡ä»¶ï¼Œå®Œå…¨é¿å…æ•°æ®é›†å¤„ç†é—®é¢˜"""
    text_content = """
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚
æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ¨¡å¼è¯†åˆ«ã€‚
è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚
å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å¾ˆå¼ºå¤§ã€‚
æ¨¡å‹å¾®è°ƒå¯ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡éœ€æ±‚ã€‚
äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚
ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»æµ·é‡æ–‡æœ¬ä¸­å­¦ä¹ ã€‚
æ³¨æ„åŠ›æœºåˆ¶æé«˜æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬èƒ½åŠ›ã€‚
Transformeræ¶æ„æ˜¯ç°ä»£NLPçš„åŸºç¡€ã€‚
""" * 100  # é‡å¤åˆ›å»ºè¶³å¤Ÿå†…å®¹
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('train_data.txt', 'w', encoding='utf-8') as f:
        f.write(text_content)
    
    print(f"ğŸ“ åˆ›å»ºè®­ç»ƒæ–‡ä»¶ï¼Œå¤§å°: {len(text_content)} å­—ç¬¦")

def main():
    """ä½¿ç”¨æœ€ç¨³å®šã€æœ€ç®€å•çš„æ–¹æ³•"""
    print("ğŸš€ æœ€å°åŒ–å·¥ä½œç‰ˆæœ¬")
    print("=" * 40)
    
    # 1. åˆ›å»ºè®­ç»ƒæ•°æ®æ–‡ä»¶
    create_text_file()
    
    # 2. ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡å‹å’Œtokenizer
    print("ğŸ”§ åŠ è½½GPT-2æ¨¡å‹...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print("âœ… è®¾ç½®pad_token")
    
    # 3. ä½¿ç”¨TextDatasetï¼ˆæœ€ç¨³å®šçš„æ–¹å¼ï¼‰
    print("ğŸ“Š åˆ›å»ºTextDataset...")
    train_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path="train_data.txt",
        block_size=128  # åºåˆ—é•¿åº¦
    )
    
    # 4. æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å› æœè¯­è¨€å»ºæ¨¡
    )
    
    # 5. è®­ç»ƒå‚æ•°ï¼ˆæœ€ç®€é…ç½®ï¼‰
    training_args = TrainingArguments(
        output_dir="./minimal_output",
        overwrite_output_dir=True,
        num_train_epochs=1,
        per_device_train_batch_size=2,
        save_steps=50,
        save_total_limit=2,
        logging_steps=10,
        prediction_loss_only=True,
    )
    
    # 6. è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )
    
    # 7. è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # 8. ä¿å­˜
    trainer.save_model()
    print("âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åˆ°: ./minimal_output")
    
    # æ¸…ç†
    if os.path.exists("train_data.txt"):
        os.remove("train_data.txt")
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

if __name__ == "__main__":
    main()