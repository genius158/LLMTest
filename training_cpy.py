# ultimate_fix.py
import torch
from transformers import (
    AutoTokenizer, 
    TextDataset,
    LineByLineTextDataset,
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset
from inspect_tokenized_dataset import TokenizedDatasetInspector

from peft import LoraConfig, get_peft_model, TaskType
import json
from typing import List, Dict, Any
import os
import logging
import numpy as np
import pandas as pd


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
train_path = "./simple_train.txt"
model_name ="./models/Qwen3-1.7B"
# model_name ="./ultra_safe_model"

def prepare_line_based_dataset(tokenizer, file_path: str, max_length: int = 128) -> Dataset:
    """å‡†å¤‡åŸºäºè¡Œçš„è®­ç»ƒæ•°æ®é›†"""
    logger.info(f"ğŸ“š å‡†å¤‡è¡Œçº§è®­ç»ƒæ•°æ®: {file_path}")
        
    # åŠ è½½æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼‰
    dataset = load_dataset('text', data_files={'train': train_path})['train']
        
    # Tokenizeå‡½æ•°
    def tokenize_function(examples):
        logger.info(f"ğŸ“š tokenize_function: {examples}")
        # å¯¹æ¯è¡Œç‹¬ç«‹tokenize
        tokenized = tokenizer(
            examples['text'],
            truncation=True,      # æˆªæ–­åˆ°max_length
            padding="max_length", # å¡«å……åˆ°max_lengthï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
            max_length=128,
            return_tensors="pt"   # è¿”å›PyTorchå¼ é‡
        )
        return tokenized
        
    # åº”ç”¨tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=10,
        remove_columns=dataset.column_names,
        desc="Tokenizing lines for LM"
    )
        
    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(tokenized_dataset)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return tokenized_dataset


def get_data_from_cvs(tokenizer) -> Dataset:
    try:
        # 1. è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv("./tran_data.csv")
        print(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")
        print(f"æ•°æ®åˆ—: {df.columns.tolist()}")
        
        # 2. æ£€æŸ¥å¿…è¦åˆ—
        if 'prompt' not in df.columns or 'response' not in df.columns:
            raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å«'prompt'å’Œ'response'åˆ—")
        
        # 3. åˆ›å»ºè®­ç»ƒæ–‡æœ¬
        train_texts = []
        for _, row in df.iterrows():
            # åˆ›å»ºæ ¼å¼åŒ–çš„å¯¹è¯æ–‡æœ¬
            text = f"ç”¨æˆ·: {row['prompt']}\nåŠ©æ‰‹: {row['response']}"
            train_texts.append({"text": text})
        
        # 4. åˆ›å»ºDataset
        dataset = Dataset.from_list(train_texts)
        print(f"åˆ›å»ºDatasetï¼Œå…± {len(dataset)} æ¡æ ·æœ¬")
        
        # 5. Tokenizeå‡½æ•°
        def tokenize_function(examples):
            """
            åˆ†è¯å¤„ç†å‡½æ•°
            """
            # Tokenizeæ–‡æœ¬
            tokenized = tokenizer(
                examples["text"],
                truncation=True,      # æˆªæ–­åˆ°max_length
                padding="max_length", # å¡«å……åˆ°max_lengthï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
                max_length=128,
                return_tensors="pt"   # è¿”å›PyTorchå¼ é‡
            )
            
            # å¯¹äºè¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œlabelsé€šå¸¸æ˜¯input_idsçš„å‰¯æœ¬
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        
        # 6. åº”ç”¨åˆ†è¯å‡½æ•°
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=10,
            remove_columns=["text"]  # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—ï¼ŒèŠ‚çœå†…å­˜
        )
        
        print(f"åˆ†è¯å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")
        return tokenized_dataset
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")
        raise
    except Exception as e:
        print(f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise


# 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
def load_and_format_data(file_path):
    """è¯»å–JSONLæ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ•°æ®é›†"""
    data = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():  # è·³è¿‡ç©ºè¡Œ
                data.append(json.loads(line.strip()))
    
    # è½¬æ¢ä¸ºhuggingfaceæ•°æ®é›†æ ¼å¼
    dataset = Dataset.from_list(data)
    return dataset

# 2. æ•°æ®æ ¼å¼åŒ–å‡½æ•°
def format_conversations(example):
    """æ ¼å¼åŒ–å¯¹è¯æ•°æ®ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    # Qwen3çš„å¯¹è¯æ ¼å¼
    formatted_text = ""
    for i, message in enumerate(example["conversations"]):
        role = message['role']
        content = message['content']
        
        if role == 'user':
            formatted_text += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == 'assistant':
            formatted_text += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        elif role == 'system':
            formatted_text += f"<|im_start|>system\n{content}<|im_end|>\n"
    
    logger.info(f"formatted_text: {formatted_text}")
    return {"text": formatted_text}


def ultra_safe_tokenize_and_train():
    """è¶…å®‰å…¨çš„tokenizationå’Œè®­ç»ƒæµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹è¶…å®‰å…¨è®­ç»ƒæµç¨‹")
    print("=" * 60)
    
    output_dir = "./ultra_safe_model"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # 1. ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡å‹
        logger.info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        # 2. é…ç½®LoRA
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # LoRAç§©
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"],  # æ ‡å‡†Transformeræ¨¡å—
            bias="none",
            inference_mode=False,
        )
        model = get_peft_model(model, lora_config)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        logger.info("ğŸ“Š åˆ›å»ºTextDataset...")
        
        # åŠ è½½å¹¶å¤„ç†æ•°æ®
        print("åŠ è½½è®­ç»ƒæ•°æ®...")
        dataset = load_and_format_data("lora_identity_minimind.jsonl")
        
        # åº”ç”¨æ ¼å¼åŒ–å‡½æ•°
        dataset = dataset.map(format_conversations)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        print("åˆ†å‰²æ•°æ®é›†...")
        dataset_dict = dataset.train_test_split(test_size=0.1, seed=42)
        
        # æ£€æŸ¥åˆ†å‰²ç»“æœ
        print(f"è®­ç»ƒé›†å¤§å°: {len(dataset_dict['train'])}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(dataset_dict['test'])}")
        
        # 7. åˆ†è¯å‡½æ•°
        def tokenize_function(examples):
            """åˆ†è¯å‡½æ•°"""
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=256,
                return_tensors=None  # è¿”å›æ™®é€šå­—å…¸è€Œä¸æ˜¯tensors
            )
        
        # 8. åˆ†åˆ«å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œåˆ†è¯
        print("åˆ†è¯å¤„ç†...")
        
        # åˆ†è¯è®­ç»ƒé›†
        tokenized_train = dataset_dict["train"].map(
            tokenize_function,
            batched=True,
            remove_columns=["text", "conversations"]  # ç§»é™¤åŸå§‹åˆ—
        )
        
        # åˆ†è¯æµ‹è¯•é›†
        tokenized_test = dataset_dict["test"].map(
            tokenize_function,
            batched=True,
            remove_columns=["text", "conversations"]  # ç§»é™¤åŸå§‹åˆ—
        )
        
        print(f"åˆ†è¯åè®­ç»ƒé›†ç‰¹å¾: {tokenized_train.features}")
        print(f"åˆ†è¯åæµ‹è¯•é›†ç‰¹å¾: {tokenized_test.features}")
    
        # 9. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
        if len(tokenized_train) == 0:
            print("é”™è¯¯: è®­ç»ƒé›†ä¸ºç©º!")
            return None, None
        
        # 5. é…ç½®æ•°æ®æ•´ç†å™¨ï¼ˆå…³é”®ï¼šè®©DataCollatorå¤„ç†labelsï¼‰
        logger.info("ğŸ”§ é…ç½®DataCollator...")
        # å–‚ç»™æ¨¡å‹è®­ç»ƒçš„æ ‡å‡†æ‰¹æ¬¡æ•°æ®å­—å…¸ï¼ˆé€šå¸¸åŒ…å« input_idsï¼Œ attention_maskï¼Œ labelsï¼‰
        # DataCollatorForLanguageModelingå¯ä»¥è‡ªåŠ¨å¤„ç†
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,  # å› æœè¯­è¨€å»ºæ¨¡
        )
        
        # 6. è®­ç»ƒå‚æ•°ï¼ˆæœ€ç®€é…ç½®ï¼‰
        training_args = TrainingArguments(
            output_dir=output_dir,
            overwrite_output_dir=True,
            num_train_epochs=2, #è®­ç»ƒè½®æ¬¡
            per_device_train_batch_size=2,
            gradient_accumulation_steps=2,
            learning_rate=1e-4, #é€šç”¨å­¦ä¹ ç‡
            save_steps=30,
            save_total_limit=1,
            weight_decay=0.01, 
            logging_steps=30,
            remove_unused_columns=False, #åˆ é™¤æ— æ•ˆçš„ç»„
            dataloader_pin_memory=False,
            # ç¦ç”¨å¯èƒ½å¼•èµ·é—®é¢˜çš„åŠŸèƒ½
            prediction_loss_only=True,
        )
        
        # 7. åˆ›å»ºè®­ç»ƒå™¨
        logger.info("ğŸ¯ åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_test,
            tokenizer=tokenizer,
        )
        
        # 8. å¼€å§‹è®­ç»ƒ
        logger.info("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
        print("=" * 50)
        
        train_result = trainer.train()
        
        # 9. ä¿å­˜æ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        metrics = train_result.metrics
        logger.info("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        logger.info(f"ğŸ“Š æœ€ç»ˆæŸå¤±: {metrics.get('train_loss', 'N/A')}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹----")

    # è¿è¡Œè¶…å®‰å…¨è®­ç»ƒ
    success = ultra_safe_tokenize_and_train()
    
    if success:
        print("\n è®­ç»ƒæˆåŠŸ")
    else:
        print("\n è®­ç»ƒå¤±è´¥")