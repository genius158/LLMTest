# advanced_validation.py
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import logging
from pathlib import Path
import json
from typing import List, Dict, Any, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('model_validation.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class ModelValidator:
    """æ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path: str, device: str = None):
        self.model_path = model_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        
    def check_model_files(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
        logger.info("ğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§...")
        
        required_files = [
            'config.json',
            'pytorch_model.bin',  # PyTorchæ¨¡å‹æ–‡ä»¶
            'tokenizer_config.json',
            'special_tokens_map.json',
            'vocab.json'  # å¯¹äºæŸäº›tokenizer
        ]
        
        # å¯é€‰çš„æ–‡ä»¶
        optional_files = [
            'generation_config.json',
            'model.safetensors',  # å®‰å…¨æ ¼å¼
            'tokenizer.model'  # å¯¹äºsentencepiece
        ]
        
        existing_files = os.listdir(self.model_path) if os.path.exists(self.model_path) else []
        
        logger.info(f"æ¨¡å‹ç›®å½•: {self.model_path}")
        logger.info(f"æ‰¾åˆ° {len(existing_files)} ä¸ªæ–‡ä»¶")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        missing_files = []
        for file in required_files:
            if file not in existing_files:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ›¿ä»£æ–‡ä»¶
                if file == 'pytorch_model.bin' and 'model.safetensors' in existing_files:
                    logger.info("âœ… æ‰¾åˆ° model.safetensors (æ›¿ä»£ pytorch_model.bin)")
                elif file == 'vocab.json' and 'tokenizer.model' in existing_files:
                    logger.info("âœ… æ‰¾åˆ° tokenizer.model (æ›¿ä»£ vocab.json)")
                else:
                    missing_files.append(file)
        
        if missing_files:
            logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")
            return False
        
        logger.info("âœ… æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        for file in existing_files:
            file_path = os.path.join(self.model_path, file)
            if os.path.isfile(file_path):
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                logger.info(f"  {file}: {size_mb:.1f} MB")
        
        return True
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        logger.info(f"ğŸ¤– åŠ è½½æ¨¡å‹...")
        
        try:
            # 1. åŠ è½½tokenizer
            logger.info("  åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True  # å¯¹äºæŸäº›è‡ªå®šä¹‰æ¨¡å‹
            )
            
            # ç¡®ä¿pad_tokenè®¾ç½®æ­£ç¡®
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"  âœ… TokenizeråŠ è½½æˆåŠŸ")
            logger.info(f"    è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
            logger.info(f"    æ¨¡å‹æœ€å¤§é•¿åº¦: {self.tokenizer.model_max_length}")
            
            # 2. åŠ è½½æ¨¡å‹
            logger.info("  åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.model = self.model.to(self.device)
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            logger.info(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"    è®¾å¤‡: {self.device}")
            logger.info(f"    å‚æ•°é‡: {self.model.num_parameters():,}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def test_text_generation(self, prompts: List[str] = None) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
        logger.info("ğŸ¯ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        
        if prompts is None:
            prompts = [
                "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
                "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
                "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š",
                "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—ï¼š",
                "Pythonçš„åˆ—è¡¨æ¨å¯¼å¼æ€ä¹ˆå†™ï¼Ÿ"
            ]
        
        results = {}
        
        for i, prompt in enumerate(prompts):
            logger.info(f"\nğŸ“ æµ‹è¯• {i+1}: '{prompt}'")
            
            try:
                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                
                # ç”Ÿæˆå‚æ•°
                generate_kwargs = {
                    "input_ids": inputs.input_ids,
                    "max_new_tokens": 50,  # ç”Ÿæˆ50ä¸ªæ–°token
                    "num_return_sequences": 1,
                    "temperature": 0.7,
                    "do_sample": True,
                    "top_p": 0.9,
                    "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
                }
                
                # å¦‚æœè¾“å…¥æœ‰attention_maskï¼Œæ·»åŠ 
                if "attention_mask" in inputs:
                    generate_kwargs["attention_mask"] = inputs.attention_mask
                
                # ç”Ÿæˆæ–‡æœ¬
                with torch.no_grad():
                    outputs = self.model.generate(**generate_kwargs)
                
                # è§£ç ç”Ÿæˆç»“æœ
                generated = self.tokenizer.decode(
                    outputs[0], 
                    skip_special_tokens=True
                )
                
                # è®¡ç®—ç”Ÿæˆé•¿åº¦
                input_length = len(inputs.input_ids[0])
                generated_length = len(outputs[0])
                new_tokens = generated_length - input_length
                
                logger.info(f"  è¾“å…¥é•¿åº¦: {input_length} tokens")
                logger.info(f"  è¾“å‡ºé•¿åº¦: {generated_length} tokens")
                logger.info(f"  ç”Ÿæˆ {new_tokens} ä¸ªæ–°token")
                logger.info(f"  ç”Ÿæˆç»“æœ: {generated}")
                
                results[f"test_{i+1}"] = {
                    "prompt": prompt,
                    "generated": generated,
                    "input_tokens": input_length,
                    "output_tokens": generated_length,
                    "new_tokens": new_tokens,
                    "success": True
                }
                
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                results[f"test_{i+1}"] = {
                    "prompt": prompt,
                    "error": str(e),
                    "success": False
                }
        
        return results
    
    def calculate_perplexity(self, test_texts: List[str], batch_size: int = 1) -> Optional[float]:
        """è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰"""
        logger.info("ğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
        
        if not test_texts:
            logger.warning("âš ï¸ æ²¡æœ‰æµ‹è¯•æ–‡æœ¬ï¼Œè·³è¿‡å›°æƒ‘åº¦è®¡ç®—")
            return None
        
        self.model.eval()
        total_loss = 0.0
        total_tokens = 0
        
        try:
            with torch.no_grad():
                for i in range(0, len(test_texts), batch_size):
                    batch_texts = test_texts[i:i+batch_size]
                    
                    # æ‰¹é‡ç¼–ç 
                    inputs = self.tokenizer(
                        batch_texts, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True, 
                        max_length=512
                    ).to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(
                        **inputs, 
                        labels=inputs.input_ids
                    )
                    
                    # ç´¯è®¡æŸå¤±å’Œtokenæ•°
                    batch_loss = outputs.loss.item()
                    batch_tokens = inputs.input_ids.numel()
                    
                    total_loss += batch_loss * batch_tokens
                    total_tokens += batch_tokens
                    
                    if (i // batch_size) % 10 == 0:
                        logger.info(f"  å·²å¤„ç† {min(i+batch_size, len(test_texts))}/{len(test_texts)} ä¸ªæ–‡æœ¬")
            
            if total_tokens > 0:
                avg_loss = total_loss / total_tokens
                perplexity = torch.exp(torch.tensor(avg_loss))
                logger.info(f"âœ… å¹³å‡æŸå¤±: {avg_loss:.4f}")
                logger.info(f"âœ… å›°æƒ‘åº¦ (PPL): {perplexity:.2f}")
                return perplexity.item()
            else:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„tokenç”¨äºè®¡ç®—å›°æƒ‘åº¦")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—å›°æƒ‘åº¦å¤±è´¥: {e}")
            return None
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        logger.info("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨...")
        
        try:
            # è·å–æ¨¡å‹å‚æ•°æ•°é‡
            num_params = self.model.num_parameters()
            
            # ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆå‡è®¾float32ï¼‰
            model_size_mb = (num_params * 4) / (1024 * 1024)  # 4 bytes per float32
            
            # GPUå†…å­˜ä¿¡æ¯
            if self.device == "cuda":
                allocated = torch.cuda.memory_allocated() / (1024 * 1024)
                reserved = torch.cuda.memory_reserved() / (1024 * 1024)
                logger.info(f"  GPUå·²åˆ†é…å†…å­˜: {allocated:.1f} MB")
                logger.info(f"  GPUä¿ç•™å†…å­˜: {reserved:.1f} MB")
            
            logger.info(f"  æ¨¡å‹å‚æ•°é‡: {num_params:,}")
            logger.info(f"  ä¼°ç®—æ¨¡å‹å¤§å°: {model_size_mb:.1f} MB")
            
            return {
                "num_params": num_params,
                "estimated_size_mb": model_size_mb,
                "device": self.device,
            }
            
        except Exception as e:
            logger.error(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
            return {}
    
    def analyze_model_config(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹é…ç½®"""
        logger.info("âš™ï¸ åˆ†ææ¨¡å‹é…ç½®...")
        
        try:
            config_path = os.path.join(self.model_path, "config.json")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # æå–å…³é”®é…ç½®
            important_keys = [
                "vocab_size", "hidden_size", "num_hidden_layers",
                "num_attention_heads", "intermediate_size",
                "max_position_embeddings", "model_type"
            ]
            
            config_info = {}
            for key in important_keys:
                if key in config:
                    config_info[key] = config[key]
                    logger.info(f"  {key}: {config[key]}")
            
            return config_info
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®åˆ†æå¤±è´¥: {e}")
            return {}
    
    def run_comprehensive_validation(self, test_texts: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢éªŒè¯"""
        logger.info("ğŸ” å¼€å§‹å…¨é¢æ¨¡å‹éªŒè¯")
        print("=" * 60)
        
        validation_results = {
            "model_path": self.model_path,
            "device": self.device,
            "checks_passed": [],
            "checks_failed": [],
            "metrics": {}
        }
        
        # 1. æ£€æŸ¥æ–‡ä»¶
        if self.check_model_files():
            validation_results["checks_passed"].append("file_check")
        else:
            validation_results["checks_failed"].append("file_check")
            return validation_results
        
        # 2. åŠ è½½æ¨¡å‹
        if self.load_model():
            validation_results["checks_passed"].append("model_load")
        else:
            validation_results["checks_failed"].append("model_load")
            return validation_results
        
        # 3. åˆ†æé…ç½®
        config_info = self.analyze_model_config()
        validation_results["config"] = config_info
        
        # 4. æµ‹è¯•å†…å­˜
        memory_info = self.test_memory_usage()
        validation_results["memory"] = memory_info
        
        # 5. æµ‹è¯•ç”Ÿæˆ
        generation_results = self.test_text_generation()
        validation_results["generation"] = generation_results
        
        # 6. è®¡ç®—å›°æƒ‘åº¦
        if test_texts:
            perplexity = self.calculate_perplexity(test_texts)
            if perplexity:
                validation_results["metrics"]["perplexity"] = perplexity
                validation_results["checks_passed"].append("perplexity_calculation")
            else:
                validation_results["checks_failed"].append("perplexity_calculation")
        
        # æ€»ç»“
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ éªŒè¯æ€»ç»“:")
        logger.info(f"  é€šè¿‡æ£€æŸ¥: {len(validation_results['checks_passed'])} é¡¹")
        logger.info(f"  å¤±è´¥æ£€æŸ¥: {len(validation_results['checks_failed'])} é¡¹")
        
        if validation_results["checks_failed"]:
            logger.error(f"âŒ å¤±è´¥é¡¹: {validation_results['checks_failed']}")
        
        return validation_results
    
    def save_validation_report(self, results: Dict[str, Any], output_path: str = "validation_report.json"):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def load_test_texts(file_path: str, max_lines: int = 100) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½æµ‹è¯•æ–‡æœ¬"""
    if not os.path.exists(file_path):
        logger.warning(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    
    texts = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= max_lines:
                    break
                line = line.strip()
                if line and len(line) > 10:  # è¿‡æ»¤ç©ºè¡Œå’Œå¤ªçŸ­çš„æ–‡æœ¬
                    texts.append(line)
        
        logger.info(f"âœ… ä» {file_path} åŠ è½½äº† {len(texts)} æ¡æµ‹è¯•æ–‡æœ¬")
        return texts
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æµ‹è¯•æ–‡æœ¬å¤±è´¥: {e}")
        return []


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜çº§æ¨¡å‹éªŒè¯å·¥å…·")
    print("=" * 60)
    
    # é…ç½®
    model_path = "./ultra_safe_model"  # ä¿®æ”¹ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„
    test_file_path = "./simple_train.txt"  # ç”¨äºè®¡ç®—å›°æƒ‘åº¦çš„æµ‹è¯•æ–‡ä»¶
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        logger.info("ğŸ’¡ è¯·æ£€æŸ¥è·¯å¾„ï¼Œæˆ–è¿è¡Œè®­ç»ƒè„šæœ¬å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = ModelValidator(model_path)
    
    # åŠ è½½æµ‹è¯•æ–‡æœ¬
    test_texts = load_test_texts(test_file_path)
    
    # è¿è¡Œå…¨é¢éªŒè¯
    results = validator.run_comprehensive_validation(test_texts)
    
    # ä¿å­˜æŠ¥å‘Š
    validator.save_validation_report(results)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    if not results["checks_failed"]:
        print("ğŸ‰ æ¨¡å‹éªŒè¯é€šè¿‡!")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"ğŸ“Š ç”Ÿæˆæµ‹è¯•: {len(results.get('generation', {}))} é¡¹")
        
        if "perplexity" in results.get("metrics", {}):
            print(f"ğŸ“ˆ å›°æƒ‘åº¦: {results['metrics']['perplexity']:.2f}")
    else:
        print("âŒ æ¨¡å‹éªŒè¯å¤±è´¥")
        print(f"å¤±è´¥é¡¹: {results['checks_failed']}")
    
    print("=" * 60)


if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤§è¯­è¨€æ¨¡å‹éªŒè¯å·¥å…·")
    parser.add_argument("--model_path", type=str, default="./ultra_safe_model", 
                       help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--test_file", type=str, default="./simple_train.txt",
                       help="æµ‹è¯•æ–‡æœ¬æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, choices=["cpu", "cuda", "auto"], 
                       default="auto", help="è¿è¡Œè®¾å¤‡")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ¨¡å‹è·¯å¾„
    model_path = args.model_path
    test_file_path = args.test_file
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    # è¿è¡Œä¸»å‡½æ•°
    main()