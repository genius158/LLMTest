# inspect_tokenized_dataset.py
from datasets import Dataset
from transformers import AutoTokenizer
import logging
from typing import List, Dict, Any
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TokenizedDatasetInspector:
    """tokenized_dataset å†…å®¹æŸ¥çœ‹å™¨"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def basic_inspection(self, tokenized_dataset: Dataset, num_samples: int = 5):
        """åŸºç¡€æŸ¥çœ‹æ–¹æ³•"""
        print("ğŸ” tokenized_dataset åŸºç¡€æŸ¥çœ‹")
        print("=" * 50)
        
        # 1. åŸºæœ¬ä¿¡æ¯
        print("ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ ·æœ¬æ•°é‡: {len(tokenized_dataset):,}")
        print(f"   ç‰¹å¾åˆ—: {tokenized_dataset.column_names}")
        
        # 2. æ•°æ®ç»“æ„
        if len(tokenized_dataset) > 0:
            sample = tokenized_dataset[0]
            print(f"   æ ·æœ¬ç»“æ„: {type(sample)}")
            if isinstance(sample, dict):
                print(f"   æ ·æœ¬é”®: {list(sample.keys())}")
                for key, value in sample.items():
                    if hasattr(value, 'shape'):
                        print(f"     {key}: å½¢çŠ¶ {value.shape}")
                    elif isinstance(value, list):
                        print(f"     {key}: é•¿åº¦ {len(value)}")
                    else:
                        print(f"     {key}: {type(value)}")
        
        # 3. æŸ¥çœ‹å‰å‡ ä¸ªæ ·æœ¬
        print(f"\nğŸ“„ å‰ {num_samples} ä¸ªæ ·æœ¬:")
        for i in range(min(num_samples, len(tokenized_dataset))):
            self._print_sample_details(tokenized_dataset, i, f"æ ·æœ¬ {i+1}")
        
        return tokenized_dataset
    
    def _print_sample_details(self, dataset: Dataset, index: int, title: str = "æ ·æœ¬"):
        """æ‰“å°æ ·æœ¬è¯¦æƒ…"""
        sample = dataset[index]
        print(f"\nğŸ¯ {title} (ç´¢å¼• {index}):")
        
        if isinstance(sample, dict):
            for key, value in sample.items():
                if key == 'input_ids' and hasattr(self, 'tokenizer'):
                    # è§£ç  token IDs
                    try:
                        decoded_text = self.tokenizer.decode(value, skip_special_tokens=True)
                        print(f"   {key}: {len(value)} tokens")
                        print(f"      å†…å®¹: {decoded_text[:100]}{'...' if len(decoded_text) > 100 else ''}")
                    except:
                        print(f"   {key}: {value} (æ— æ³•è§£ç )")
                elif isinstance(value, list):
                    print(f"   {key}: é•¿åº¦ {len(value)}")
                    if len(value) > 0 and isinstance(value[0], (int, float)):
                        print(f"      å‰5ä¸ªå€¼: {value[:5]}{'...' if len(value) > 5 else ''}")
                else:
                    print(f"   {key}: {value}")
        else:
            print(f"   {sample}")
    
    def statistical_analysis(self, tokenized_dataset: Dataset):
        """ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
        print("=" * 40)
        
        if len(tokenized_dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©º")
            return
        
        stats = {}
        
        # token é•¿åº¦åˆ†æ
        if 'input_ids' in tokenized_dataset.column_names:
            lengths = [len(sample['input_ids']) for sample in tokenized_dataset]
            stats['token_lengths'] = {
                'min': min(lengths),
                'max': max(lengths),
                'mean': sum(lengths) / len(lengths),
                'std': (sum((x - sum(lengths)/len(lengths))**2 for x in lengths) / len(lengths))**0.5
            }
            
            print("ğŸ“ Token é•¿åº¦ç»Ÿè®¡:")
            print(f"   æœ€çŸ­: {stats['token_lengths']['min']} tokens")
            print(f"   æœ€é•¿: {stats['token_lengths']['max']} tokens")
            print(f"   å¹³å‡: {stats['token_lengths']['mean']:.1f} tokens")
            print(f"   æ ‡å‡†å·®: {stats['token_lengths']['std']:.1f} tokens")
            
            # é•¿åº¦åˆ†å¸ƒ
            length_bins = [0, 10, 20, 50, 100, 200, 500, 1000, float('inf')]
            length_distribution = {}
            for i in range(len(length_bins)-1):
                count = sum(1 for length in lengths if length_bins[i] <= length < length_bins[i+1])
                if count > 0:
                    length_distribution[f"{length_bins[i]}-{length_bins[i+1]}"] = count
            
            print("   é•¿åº¦åˆ†å¸ƒ:")
            for range_str, count in length_distribution.items():
                percentage = count / len(lengths) * 100
                print(f"     {range_str}: {count} æ ·æœ¬ ({percentage:.1f}%)")
        
        return stats
    
    def decode_and_display(self, tokenized_dataset: Dataset, num_samples: int = 3):
        """è§£ç å¹¶æ˜¾ç¤ºåŸå§‹æ–‡æœ¬"""
        print("\nğŸ”¤ è§£ç æ˜¾ç¤ºåŸå§‹æ–‡æœ¬")
        print("=" * 40)
        
        if 'input_ids' not in tokenized_dataset.column_names:
            print("âŒ æ•°æ®é›†ä¸åŒ…å« input_ids")
            return
        
        for i in range(min(num_samples, len(tokenized_dataset))):
            sample = tokenized_dataset[i]
            input_ids = sample['input_ids']
            
            # è§£ç 
            try:
                decoded_text = self.tokenizer.decode(input_ids, skip_special_tokens=True)
                original_text = decoded_text
                
                print(f"\nğŸ“– æ ·æœ¬ {i+1}:")
                print(f"   Tokenæ•°é‡: {len(input_ids)}")
                print(f"   å†…å®¹: {original_text}")
                
                # æ˜¾ç¤ºç‰¹æ®Štoken
                if 'special_tokens_mask' in sample:
                    special_count = sum(sample['special_tokens_mask'])
                    print(f"   ç‰¹æ®Štokenæ•°é‡: {special_count}")
                
            except Exception as e:
                print(f"âŒ è§£ç å¤±è´¥: {e}")